# Bases de datos vectoriales

## Presentaci√≥n

Problemente ya hayas oido hablar de las bases de datos vectoriales, o incluso est√©s trasteando con alguna. 

Qu√© son?

Las bases de datos vectoriales son aquellas que almacenan representaciones vectoriales de datos (embeddings).

Diferencia con las cl√°sicas

Lo que las hace diferentes a las de tipo relacional, ya que, en lugar de guardar directamente los datos (num√©ricos, texto, boolenaos, fecha y hora y binarios) en tablas, los transforman y almancenan como vectores, que no son otra cosa que listas de n√∫meros de longitud fija.

Y si la forma de guardar los datos es distinta, la forma cosultarlos tambi√©n los es. Las relacionales lo hacen a trav√©s de b√∫squeda exacta pero las vectoriales usan la busqueda por similitud.

Prop√≥sito

Y es aqu√≠ d√≥nde radica la importancia que las vectoriales est√°n adquiriendo, ya que la similitud permite dotar de sem√°ntica a los datos, lo que las convierte en fuentes documentales √≥ptimas para los LLMs. Realmente su desarrollo ha sido parejo y la forma en que se trnasforman datos en vectores es atrav√©s de modelos de lenguaje desarrollados adhoc para esta tarea.












 Si las bases de datos relacionales almacenan datos (num√©ricos, texto, boolenaos, fecha y hora y binarios) en tablas relacionadas entre s√≠, las vectoriales transforman los datos en vectores

## Qu√© son? (diferencia con las cl√°sicas)

Un vectorstore (almac√©n de vectores) es una base de datos optimizada para almacenar, indexar y buscar representaciones vectoriales de datos, com√∫nmente llamadas embeddings. Es un componente clave en los sistemas que utilizan retrievers basados en embeddings, como los de Retrieval-Augmented Generation (RAG) en LLMs.

M√°s habituales: FAISS (Facebook AI Similarity Search), Milvus, Weaviate, Pinecone, ChromaDB y tambi√©n Elasticsearch, si tiene la b√∫squeda de vectores habilitada.

Ideal para b√∫squedas sem√°nticas, recomendadores, sistemas de clasificaci√≥n de texto, visi√≥n por computadora, modelos de lenguaje, etc.

Es un sistema optimizado para almacenar y recuperar vectores de alta dimensi√≥n mediante b√∫squedas por similitud.

A diferencia de bases de datos relacionales, que buscan coincidencias exactas, estas permiten encontrar resultados similares bas√°ndose en m√©tricas como la distancia coseno, euclidiana o producto escalar.
Son fundamentales en sistemas RAG, motores de b√∫squeda sem√°nticos, chatbots y recomendaci√≥n de contenido.

datos structurados vs. embeddings vectoriales
b√∫squedas exactas, filtros en columnas y relaciones entre tablas vs. ANN (Approximate Nearest Neighbor)
tablas vs. vectores de alta dimensi√≥n

| Caracter√≠stica       | Base de datos tradicional | Base de datos vectorial                        |
|----------------------|---------------------------|------------------------------------------------|
| Tipo de datos        | Estructurados (tablas, columnas) | Vectores de alta dimensi√≥n                      |
| Consultas            | Exactas, basadas en SQL   | Similitud, b√∫squeda de vecinos cercanos        |
| √çndices              | B-tree, hash indexes      | HNSW, IVF, Annoy                               |
| Escalabilidad        | Relacional, transacciones ACID | B√∫squedas aproximadas r√°pidas en grandes vol√∫menes |
| Casos de uso         | Gesti√≥n de datos estructurados | B√∫squedas sem√°nticas, IA, ML, Recomendaci√≥n    |
| Ejemplos             | MySQL, PostgreSQL, SQL Server | ChromaDB, Pinecone, FAISS, Qdrant              |

### √çndices invertidos

Un √≠ndice invertido es una estructura de datos que almacena un mapeo de t√©rminos (palabras) a los documentos en los que aparecen.

| Token           | ID de documento      |
| --------------- | -------------------- |
| London          | 1,3,8,12,23,88       |
| Paris           | 1,12,88              |
| Madrid          | 3,8,12               |
| Berlin          | 12,23                |

## Prop√≥sito



## Esquema

1. Divisi√≥n del texto (chunks).
2. Generaci√≥n de embeddings: Un modelo de embeddings (por ejemplo, OpenAI Ada, Sentence Transformers, o Cohere) convierte documentos, frases o palabras en vectores de alta dimensi√≥n.
3. Almacenamiento: Estos embeddings se guardan en el vectorstore junto con metadatos opcionales.
4. Indexaci√≥n: Se organizan de manera eficiente usando estructuras como √Årboles K-D, HNSW (Hierarchical Navigable Small World) o PQ (Product Quantization) para b√∫squedas r√°pidas.
5. B√∫squeda de similitud: Cuando se realiza una consulta, se convierte en un embedding y se comparan los vectores almacenados usando m√©tricas como cosine similarity, dot product o Euclidean distance para encontrar los m√°s relevantes.
6. Devoluci√≥n de resultados: Se retornan los documentos m√°s similares para su uso en generaci√≥n de respuestas o an√°lisis.

### 1. Divisi√≥n del texto (chunks).

#### Razones

1Ô∏è‚É£ Longitud m√°xima del modelo de embeddings
Los modelos que convierten texto en vectores tienen un l√≠mite de tokens.
Ejemplo: all-MiniLM-L6-v2 de Sentence Transformers solo admite 256 tokens por entrada.
Si el texto es m√°s largo, el modelo truncar√° la entrada, perdiendo informaci√≥n.

2Ô∏è‚É£ Mejor precisi√≥n en la recuperaci√≥n
Si almacenamos textos largos como un solo embedding, las b√∫squedas pueden ser menos precisas.
Si una consulta busca informaci√≥n espec√≠fica en un p√°rrafo dentro de un documento largo, el resultado puede ser irrelevante si el embedding representa todo el documento.

3Ô∏è‚É£ Mejor rendimiento en la b√∫squeda
Si almacenamos documentos completos en una base de datos vectorial, los c√°lculos de similitud pueden ser m√°s costosos y lentos.
Fragmentos m√°s peque√±os hacen que la b√∫squeda y el ranking sean m√°s eficientes.

### 2. Embeddings

Los embeddings son representaciones num√©ricas de datos (palabras, frases, im√°genes, etc.) en un espacio vectorial. Se utilizan para capturar significado sem√°ntico y medir similitudes entre elementos.

En el contexto del procesamiento de lenguaje natural (NLP), los embeddings convierten texto en vectores de n√∫meros para que los modelos puedan procesarlo matem√°ticamente.

Los embeddings transforman palabras o frases en vectores de alta dimensi√≥n donde t√©rminos con significados similares tienen coordenadas cercanas.

Por ejemplo, en un espacio vectorial:

‚úÖ "Rey" y "Reina" estar√°n m√°s cerca entre s√≠.
‚úÖ "Perro" y "Gato" tambi√©n estar√°n m√°s cerca.
‚ùå "Rey" y "Manzana" estar√°n m√°s alejados.

Ejemplo de representaci√≥n en 3 dimensiones (simplificada):

| Palabra | Dim 1 | Dim 2 | Dim 3 |
| ------- | ----- | ----- | ----- |
| Rey     | 0.5   | 0.8   | 0.2   |
| Reina   | 0.6   | 0.7   | 0.3   |
| Perro   | 0.9   | 0.1   | 0.4   |
| Gato    | 0.85  | 0.15  | 0.45  |

Los modelos encuentran la similitud entre embeddings con m√©tricas como:

[Distancia coseno](https://es.wikipedia.org/wiki/Similitud_coseno) (la m√°s usada en NLP).
[Distancia euclidiana](https://es.wikipedia.org/wiki/Distancia_euclidiana).
[Producto escalar](https://es.wikipedia.org/wiki/Producto_escalar).

Tipos de Embeddings y Usos

| Tipo de Embedding         | Ejemplo de Modelo                | Uso Principal                                        |
| ------------------------- | -------------------------------- | ---------------------------------------------------- |
| Word Embeddings           | Word2Vec, GloVe, FastText        | Captura significado de palabras individuales         |
| Contextualized Embeddings | BERT, RoBERTa, GPT-4             | Captura significado de palabras seg√∫n el contexto    |
| Sentence Embeddings       | SBERT, OpenAI Embeddings, Cohere | B√∫squeda sem√°ntica, RAG, recomendaci√≥n de documentos |

Importante: SIEMPRE usa el mismo modelo para generar embeddings de documentos y realizar consultas ya que diferentes modelos generan diferentes vectores para la misma oraci√≥n. Esto ocurre por varios factores: Diferencias en el entrenamiento, Variaciones en la dimensionalidad y Representaci√≥n contextual captar matices m√°s sutiles

#### metadatos

üìå ¬øQu√© son los metadatos?
Los metadatos son informaci√≥n adicional que describe o proporciona contexto sobre los datos principales (en este caso, los embeddings o los documentos que has almacenado en la base de datos vectorial). En el contexto de bases de datos vectoriales y b√∫squeda sem√°ntica, los metadatos permiten enriquecer y filtrar los resultados de b√∫squeda m√°s all√° de la similitud de los vectores.

Los metadatos tambi√©n ayudan a organizar y estructurar la base de datos, permitiendo b√∫squedas m√°s eficientes y flexibles.

Filtrar resultados por fecha, agrupar resultados en base a categor√≠as o clasificar documentos seg√∫n los metadatos. Los metadatos tambi√©n son √∫tiles para llevar un registro hist√≥rico de cu√°ndo y c√≥mo se actualizaron los documentos, permitiendo recuperar versiones anteriores si es necesario.

En resumen, los metadatos son datos sobre los datos.

Ejemplos de metadatos:
ID del documento: Un identificador √∫nico para cada documento.
Categor√≠a: Por ejemplo, tecnolog√≠a, salud, ciencia.
Fecha de creaci√≥n: Fecha en la que se cre√≥ el documento.
Autor: Informaci√≥n sobre el autor del documento.
Fuente: De d√≥nde proviene el documento (URL, base de datos, etc.).

### 4. Indexaci√≥n

La indexaci√≥n es el proceso de organizar los embeddings para hacer b√∫squedas m√°s r√°pidas y eficientes.

Sin indexaci√≥n ‚Üí La b√∫squeda es una comparaci√≥n directa con todos los vectores (brute-force), lo que puede ser lento.
Con indexaci√≥n ‚Üí Se usa una estructura optimizada para reducir el n√∫mero de comparaciones necesarias.



Algunas bases de datos vectoriales hacen la indexaci√≥n de forma autom√°tica al insertar los embeddings.

‚úÖ Ejemplos:

ChromaDB ‚Üí No necesitas preocuparte por la indexaci√≥n, lo maneja internamente.
Pinecone ‚Üí Optimiza autom√°ticamente seg√∫n el tama√±o de los datos.
Weaviate y Qdrant ‚Üí Indexan los embeddings al insertarlos.
üìå Si usas estas bases, no necesitas desarrollar la indexaci√≥n manualmente.

### 4. almacenamiento

### 5. B√∫squeda

se hace a trav√©s de un retriever que transforma la consulta en un vector

#### Chroma

üîπ Configuraciones que pueden afectar la b√∫squeda
1Ô∏è‚É£ Ajustar el n√∫mero de resultados (n_results)
Si no obtienes buenos resultados, prueba con diferentes valores de n_results.

```py
results = collection.query(query_embeddings=query_embedding, n_results=10)  # M√°s resultados
```

2Ô∏è‚É£ Filtrar por metadatos (b√∫squeda h√≠brida)

Si guardaste metadatos en ChromaDB, puedes filtrar los resultados.

```py
results = collection.query(
    query_embeddings=query_embedding,
    n_results=3,
    where={"categoria": "tecnolog√≠a"}  # Solo documentos de esta categor√≠a
)
```

3Ô∏è‚É£ Usar b√∫squeda h√≠brida (texto + vectores)

Si la b√∫squeda solo con embeddings no es suficiente, puedes combinarla con b√∫squeda de texto tradicional.

```py
results = collection.query(
    query_texts=["inteligencia artificial en educaci√≥n"],  # B√∫squeda por texto
    query_embeddings=query_embedding,  # B√∫squeda por vector
    n_results=5
)
```

üìå Esto mejora la precisi√≥n combinando texto y similitud sem√°ntica.

### 6 devoluci√≥n de resultados 

